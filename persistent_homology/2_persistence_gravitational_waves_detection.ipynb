{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `giotto-tda` persistent homology tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tutorial: Hilbert embedding for gravitational waves \n",
    "\n",
    "Gravitational waves (GW) are caused by many cosmic events, such as supernovaes detection, colliding black holes, or neutron stars. While challenging due to their small amplitude, identifying GW is considered as [opening a new window on the universe](https://www.ligo.caltech.edu/page/why-detect-gw). In the literature, formally, the task is often presented as a binary classification problem, which consists of deciding whether a signal contains the signature of a GW or not. Effective approaches with CNNs have been proposed, which, however, require significant data to be trained. \n",
    "\n",
    "In [an article](https://arxiv.org/abs/1910.08245), Chrisopher Bresten and Jae-Hun Jung proposed to include topological signatures in a CNN classifier to improve the models' performance. In this tutorial, we present an example of a classification pipeline relying only on topological features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With giotto-tda, we can reproduce part of the research and address the following question. \n",
    "\n",
    "**Research question:** Would the Hilbert transform (a signal processing approach) help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_datasets import make_gravitational_waves\n",
    "from pathlib import Path\n",
    "\n",
    "R = 0.65\n",
    "n_signals = 200\n",
    "DATA = Path(\"./data\")\n",
    "\n",
    "noisy_signals, gw_signals, labels = make_gravitational_waves(\n",
    "    path_to_data=DATA, n_signals=n_signals, r_min=R, r_max=R, n_snr_values=1\n",
    ")\n",
    "\n",
    "print(f\"Number of noisy signals: {len(noisy_signals)}\")\n",
    "print(f\"Number of timesteps per series: {len(noisy_signals[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's visualise the two different types of time series that we wish to classify: one that is pure noise vs. one that is composed of noise plus an embedded gravitational wave signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# get the index corresponding to the first pure noise time series\n",
    "background_idx = np.argmin(labels)\n",
    "# get the index corresponding to the first noise + gravitational wave time series\n",
    "signal_idx = np.argmax(labels)\n",
    "\n",
    "ts_noise = noisy_signals[background_idx]\n",
    "ts_background = noisy_signals[signal_idx]\n",
    "ts_signal = gw_signals[signal_idx]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Noise\", \"Noise and GW\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(ts_noise))), y=ts_noise, mode=\"lines\", name=\"noise\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(ts_background))),\n",
    "        y=ts_background,\n",
    "        mode=\"lines\",\n",
    "        name=\"background\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(ts_signal))), y=ts_signal, mode=\"lines\", name=\"signal\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a setting simpler than in the article. We create a dataset as follows:\n",
    "\n",
    "* Generate gravitational wave signals that correspond to non-spinning binary black hole mergers\n",
    "* Generate a noisy time series and embed a gravitational wave signal with probability 0.5 at a random time.\n",
    "\n",
    "The result is a set of time series of the form\n",
    "\n",
    "$$ s = g + \\epsilon \\frac{1}{R}\\xi $$\n",
    "\n",
    "where $g$ is a gravitational wave signal from the reference set, $\\xi$ is Gaussian noise, $\\epsilon=10^{-19}$ scales the noise amplitude to the signal, and $R$ is a parameter that controls the signal-to-noise-ratio (SNR).\n",
    "\n",
    "Here, we set $R$ to $0.65$, what gives an SNR of 17.89, placing ourselves in a favorable setting, compared to the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Recurrent structure with Takens' Embedding\n",
    "Embeddings allow you to study the recurrent structure, present in a time series.\n",
    "![A 2-dimensional time delay embedding](img/time_delay_embedding.gif)\n",
    "$$\n",
    "TD_{d,\\tau} s : \\mathbb{R} \\to \\mathbb{R}^{d}\\,, \\qquad t \\to \\begin{bmatrix}\n",
    "           s(t) \\\\\n",
    "           s(t + \\tau) \\\\\n",
    "           s(t + 2\\tau) \\\\\n",
    "           \\vdots \\\\\n",
    "           s(t + (d-1)\\tau)\n",
    "         \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "More details about the Takens' embedding in the context of TDA can be found in one of the review works [Topological Time Series Analysis](http://www.ams.org/notices/201905/rnoti-p686.pdf) or in the [time-series tutorial](https://github.com/giotto-ai/giotto-tda/blob/master/examples/topology_time_series.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import SingleTakensEmbedding\n",
    "\n",
    "embedding_dimension = 30\n",
    "embedding_time_delay = 30\n",
    "stride = 5\n",
    "\n",
    "embedder = SingleTakensEmbedding(parameters_type=\"search\", n_jobs=6,\n",
    "                                 time_delay=embedding_time_delay, dimension=embedding_dimension,\n",
    "                                 stride=stride)\n",
    "\n",
    "y_gw_embedded = embedder.fit_transform(gw_signals[0]) # argument: a single time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA to project our high-dimensional space to 3-dimensions for visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "y_gw_embedded_pca = pca.fit_transform(y_gw_embedded)\n",
    "\n",
    "plot_point_cloud(y_gw_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that the decaying periodic signal generated by a black hole merger emerges as a _spiral_ in the time delay embedding space! For contrast, let's compare this to one of the pure noise time series in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noise_embedded = embedder.transform(noisy_signals[background_idx])\n",
    "y_noise_embedded_pca = pca.fit_transform(y_noise_embedded)\n",
    "\n",
    "plot_point_cloud(y_noise_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, pure noise resembles a high-dimensional ball in the time delay embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is a difference in the global structure of the data. Let us see if it is captured by persistent homology. Let's build a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import TakensEmbedding\n",
    "\n",
    "from gtda.metaestimators import CollectionTransformer\n",
    "from gtda.pipeline import Pipeline\n",
    "\n",
    "takens = TakensEmbedding(time_delay=200,\n",
    "                         dimension=10,\n",
    "                         stride=10) # argument: collection of time series\n",
    "\n",
    "batch_pca = CollectionTransformer(PCA(n_components=3), n_jobs=-1) # n_jobs\n",
    "\n",
    "takens_pipeline = Pipeline(steps=[(\"takens\", takens),\n",
    "                                  (\"pca\", batch_pca)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy, Scaler\n",
    "from gtda.homology import WeakAlphaPersistence\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "weak_alpha = WeakAlphaPersistence(homology_dimensions=[0, 1])\n",
    "\n",
    "scaling = Scaler()\n",
    "\n",
    "entropy = PersistenceEntropy(normalize=True,)\n",
    "\n",
    "logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [(\"embedder\", takens_pipeline),\n",
    "         (\"persistence\", weak_alpha),\n",
    "         (\"scaling\", scaling),\n",
    "         (\"entropy\", entropy),\n",
    "         (\"logistic\", logistic)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_classifier = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    noisy_signals, labels, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_scores\n",
    "print_scores(topological_classifier, X_train, X_valid, y_train, y_valid, show_auc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_classifier[-1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in the regression are negative, indicating that the distribution of persistence values for GW is less \"uniform\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.4 \"New\" idea: Hilbert transform map\n",
    "In signal processing, the *analytic signal representation* $H(s)(t)$ of the signal $s(t)$\n",
    "$$H(s)(t) = \\frac{1}{\\pi}p.v.\\int_{-\\infty}^{\\infty} \\frac{s(\\tau)}{t-\\tau}d\\tau,$$\n",
    "is a common denominator for many spectral methods and it often captures the recurrent nature of a system.\n",
    "In [an article](https://ieeexplore-ieee-org.proxy.scd.u-psud.fr/document/8553502), it has been proposed as a generalization of the Takens embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import hilbert\n",
    "\n",
    "def hilbert_map(x):\n",
    "    analytic_signal = hilbert(x)\n",
    "    return np.stack([np.real(analytic_signal), np.imag(analytic_signal)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "hilbert_embedding = CollectionTransformer(FunctionTransformer(func=hilbert_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of the Hilbert map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(0, 4*np.pi, 300)\n",
    "signal = np.sin(theta*2.05) + np.cos(theta*4)\n",
    "hilbert_signal = hilbert_embedding.fit_transform([signal])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Signal\", \"Analytic signal\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=theta, y=signal, mode=\"lines\", name=\"noise\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hilbert_signal[:,0], y=hilbert_signal[:,1], mode=\"lines\", name=\"signal\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it do on our point cloud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gw_hilbert = hilbert_embedding.fit_transform(gw_signals)\n",
    "y_gw_hilbert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(y_gw_hilbert[0,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar to the PCA of the TakensEmbeddnig. However, there are more circles on the outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noise_hilbert = hilbert_embedding.transform(noisy_signals)\n",
    "plot_point_cloud(y_noise_hilbert[background_idx, ::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgms = weak_alpha.fit_transform([y_gw_hilbert[0, ], y_noise_hilbert[background_idx, ]])\n",
    "dgm_gw, dgm_noise = Scaler().fit_transform(dgms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_alpha.plot([dgm_gw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_alpha.plot([dgm_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more highly-persistent points in dimension 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\"embedder\": [takens_pipeline],\n",
    "               #\"embedder__takens__dimension\": [10, ...],\n",
    "               #\"embedder__takens__time_delay\" : [200, ...],\n",
    "              },\n",
    "              {\"embedder\": [hilbert_embedding]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "grid_search_results = GridSearchCV(topological_classifier,\n",
    "                                   param_grid=param_grid,\n",
    "                                   cv=KFold(n_splits=3, shuffle=True),\n",
    "                                   scoring=\"roc_auc\",\n",
    "                                   return_train_score=True,\n",
    "                                  ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = grid_search_results.best_estimator_\n",
    "print(f\"The best score is {grid_search_results.best_score_}, achieved by:\\n{best_pipeline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further ideas\n",
    "\n",
    "1. Cross-validate the parameters of the `TakensEmbedding`\n",
    "\n",
    "2. Construct a graph from the time series: the Hilbert map is not the only idea different from the standard Takens Embedding. For example, [(A. Myers & al.)](https://arxiv.org/pdf/1904.07403.pdf) authors have proposed the permutation embedding, and construction a graph.\n",
    "\n",
    "3. Use other features - landscapes, persistence images, silhouettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

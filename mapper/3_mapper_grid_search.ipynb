{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Mapper parameters\n",
    "\n",
    "Mapper parameters are notoriously difficult to choose. Knowledge of your data can greatly narrow down the possibilities for your Mapper's filter function, covering scheme and/or clustering algorithm, but more fine-grained hyperparameters (e.g. ``eps`` in the case of ``DBSCAN``) can be difficult to guess.\n",
    "\n",
    "In this notebook, we implement an idea described in Sect. 3.1 of [N. Chalapathi's BSc thesis](https://www.cs.utah.edu/docs/techreports/2021/PDF/UUCS-21-009.pdf), namely to use the *Akaike information criterion* to optimize Mapper hyperparameters.\n",
    "\n",
    "Since ``giotto-tda`` implements Mapper as a ``scikit-learn`` pipeline, it inherits convenient frameworks for creating parameter grids and selecting the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import numpy as np\n",
    "\n",
    "# Data viz\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA magic\n",
    "from gtda.mapper import (\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    OneDimensionalCover,\n",
    "    MapperInteractivePlotter\n",
    "    )\n",
    "\n",
    "# ML tools\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = datasets.make_circles(n_samples=200, noise=0.05, factor=0.4, random_state=42)\n",
    "\n",
    "plot_point_cloud(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic(X, nodes_elements, X_idx_to_node_idxs):\n",
    "    \"\"\"Custom algorithm for computing AIC-based Mapper graph score.\"\"\"\n",
    "    ## 0. Preparations\n",
    "    #    - extract number of samples and ambient dimension\n",
    "    N, d = X.shape\n",
    "\n",
    "    ## 1. Compute the centroids of all Mapper nodes\n",
    "    centroids = np.array([np.mean(X[node_elements], axis=0)\n",
    "                          for node_elements in nodes_elements])\n",
    "\n",
    "    ## 2. Produce a hard clustering by least distance to possible centroids\n",
    "    #  2.1 Unique global cluster labels\n",
    "    labels_unique = np.empty(N, dtype=np.int64)\n",
    "    for i in range(N):\n",
    "        possible_nodes = X_idx_to_node_idxs[i]\n",
    "        if len(possible_nodes) == 1:\n",
    "            labels_unique[i] = possible_nodes[0]\n",
    "        else:\n",
    "            rel_idx = np.argmin([np.linalg.norm(X[i] - centroids[node_id])\n",
    "                                 for node_id in possible_nodes])\n",
    "            labels_unique[i] = possible_nodes[rel_idx]\n",
    "    #  2.2 List of clusters\n",
    "    clusters = []\n",
    "    for i, label in enumerate(labels_unique):\n",
    "        n_clusters_now = len(clusters)\n",
    "        to_add = label + 1 - n_clusters_now\n",
    "        if to_add > 0:\n",
    "            clusters += [[] for _ in range(to_add)]\n",
    "        clusters[label].append(i)\n",
    "    clusters = [np.array(c) for c in clusters if c]\n",
    "\n",
    "    ## 3. Compute AIC for hard clustering\n",
    "    k = len(clusters)\n",
    "    cluster_sizes = np.array(list(map(len, clusters)))\n",
    "    sigma_sq = np.sum((X - centroids[labels_unique])**2) / (d * (N - k))\n",
    "    aic = 2 * np.sum(cluster_sizes * np.log(cluster_sizes))\n",
    "    aic -= 2 * N * np.log(N) \n",
    "    aic -= N * d * np.log(2 * np.pi * sigma_sq) \n",
    "    aic -= d * (N - k)\n",
    "    aic -= 2 * k * (d + 1)\n",
    "\n",
    "    return aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AIC_score(pipeline_params):\n",
    "    \"\"\"Compute the AIC score for a given Mapper pipeline on input data X.\"\"\"\n",
    "\n",
    "    ## 1. Replace current pipeline params with new ones in `pipeline_params`, and fit\n",
    "    _pipeline = clone(pipeline)\n",
    "    _pipeline.set_params(**pipeline_params)\n",
    "    _pipeline.fit(X)\n",
    "\n",
    "    ## 2. Extract information from Mapper graph:\n",
    "    #    `node_elements`: elements of X in each node\n",
    "    #    `X_idx_to_node_ids`: a mapping\n",
    "    #        data index -> {global node IDs of all Mapper nodes the point belongs to}\n",
    "    graph = _pipeline.named_steps[\"nerve\"].graph_\n",
    "    nodes = graph.vs\n",
    "    \n",
    "    # 2.1 `node_elements`\n",
    "    nodes_elements = nodes[\"node_elements\"]\n",
    "\n",
    "    # 2.2 `X_idx_to_node_ids`\n",
    "    global_ids = {(nodes[i][\"pullback_set_label\"], nodes[i][\"partial_cluster_label\"]): i\n",
    "                  for i in range(len(nodes))}\n",
    "    labels = _pipeline.named_steps[\"clustering\"].labels_\n",
    "    X_idx_to_node_idxs = [[global_ids[tup] for tup in labels[i]]\n",
    "                          for i in range(len(X))]\n",
    "\n",
    "    # Compute AIC score and return\n",
    "    return pipeline_params, compute_aic(X, nodes_elements, X_idx_to_node_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_mapper_pipeline(filter_func=Projection(columns=1),\n",
    "                                cover=OneDimensionalCover())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"clusterer\": (HDBSCAN(), DBSCAN()),\n",
    "              \"cover__n_intervals\": (3, 5, 10, 15),\n",
    "              \"cover__overlap_frac\": (0.1, 0.2, 0.3),\n",
    "              \"cover__kind\": (\"uniform\", \"balanced\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search in parallel using all available cores\n",
    "grid_search_results = Parallel(n_jobs=-1)(\n",
    "    delayed(AIC_score)(pipeline_params)\n",
    "    for pipeline_params in ParameterGrid(param_grid)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax([r[1] for r in grid_search_results])\n",
    "best_grid, best_score = grid_search_results[best_idx]\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_params(**best_grid)\n",
    "plotter = MapperInteractivePlotter(pipeline, X)\n",
    "plotter.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
